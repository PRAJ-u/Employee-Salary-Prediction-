import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv("/content/sample_data/adult.csv")
data.head(15)

data.shape
finding null values
data.isna().sum()
print(data.workclass.value_counts())
data.workclass.replace({'?':'Others'},inplace=True)
print('Workclass')
print(data['workclass'].value_counts())
data
print(data['workclass'].value_counts())
print(data.education.value_counts())

print(data['marital-status'].value_counts())
print(data.occupation.value_counts())
data.occupation.replace({'?':'Others'},inplace=True)
Finding and handling outliers
plt.boxplot(data['age'])
plt.show()
data=data[(data['age']<=75) & (data['age']>=17)]
# Outliers removing/ dataset cleaning by mutual understanding
data.workclass.value_counts()

# in the above code 'without-pay and 'never-worked' having very less count as well as they are not earning
# so not required for income prediction
# so we can remove such data
data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']
data.shape
(48438, 15)
data.education.value_counts()
# here also we can eliminate some categories like '1st-4th','5th-6th','Preschool'
data=data[data['education']!='1st-4th']
data=data[data['education']!='5th-6th']
data=data[data['education']!='Preschool']
data.shape
(47619, 15)
# now the 'education' and 'education-num' are giving us same information so we will keep only one of them
# here i will remove the text data column 'education'
data.drop(columns=['education'],inplace=True)
data.columns

data
# as we know the algorithm is a mathematical expression so it cannot work on text data
# so we have to convert text data collumns in to numerical for that we will use encoder
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
data['workclass']=encoder.fit_transform(data['workclass'])
data['marital-status']=encoder.fit_transform(data['marital-status'])
data['occupation']=encoder.fit_transform(data['occupation'])
data['relationship']=encoder.fit_transform(data['relationship'])
data['race']=encoder.fit_transform(data['race'])
data['gender']=encoder.fit_transform(data['gender'])
data['native-country']=encoder.fit_transform(data['native-country'])
data
# spliting data in to X(independent variable) and Y (dependent variable)
X=data.drop(columns=['income'])
Y=data['income']
X
Y
# scaller will convert entire data in one range that is 0 to 1


from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X=scaler.fit_transform(X)
X
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.2,random_state=23,stratify=Y)
# here random_state=True suggest that the records getting choosen for trainning
# and testing will be in random order
# statify=Y we use only for classification task
# statify maintans the catagorical ratio in output column(Y) while getting choosen for trainning and testing
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain,ytrain)
predict=knn.predict(xtest)
predict
from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)
0.8167786644267114
# from sklearn.model_selection import GridSearchCV
# knn=KNeighborsClassifier(n_jobs=-1)
# param_grid={
#     'n_neighbors':[3,5,7],
#     'weights':['uniform', 'distance'],
#     'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],
# }
# gsv=GridSearchCV(knn,param_grid,n_jobs=-1,refit=True)
# gsv.fit(xtrain,ytrain)
# prediction=gsv.predict(xtest)
# print(gsv.best_params_)
# print(gsv.best_score_)
# print(accuracy_score(ytest,prediction))




